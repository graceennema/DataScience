{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b996294",
   "metadata": {},
   "source": [
    "### DATASCI 2000 COURSE NOTES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5bcb0f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff0abf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import scipy.optimize as so\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "%matplotlib inline\n",
    "\n",
    "# Read DF \n",
    "df = pd.read_csv(\"run10sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9d0981",
   "metadata": {},
   "outputs": [],
   "source": [
    ".mean() # Takes mean\n",
    ".std() # Takes standard deviation \n",
    ".value_counts()[\"x\"] # Number of x in a category ex. nMenS = df['gender'].value_counts()['M']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f701b2",
   "metadata": {},
   "source": [
    "## Lecture 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c487a07",
   "metadata": {},
   "source": [
    "## Lecture 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b1058e",
   "metadata": {},
   "source": [
    "Contingency Tables: summarizes data for two categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bb6708",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.crosstab(df['row'], df['column'], margins = True)\n",
    "# add normalize = \"index\" for realtive ratios\n",
    "\n",
    "# Index using \n",
    "\n",
    "table.iloc['row']['column']\n",
    "\n",
    "# Table based on a certain condition \n",
    "\n",
    "table2 = pd.crosstab(df[\"genre\"] == \"comedy\", df[\"dirGender\"], margins = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb417c8c",
   "metadata": {},
   "source": [
    "### Graph Set up - General Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b3614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Graph \n",
    "fig, axes = plt.subplots(figsize = (10,5))\n",
    "sns.violinplot(data = df, y = \"income\", x = \"gender\", ax = axes, color = \"y\")\n",
    "axes.set_ylabel(\"Income\")\n",
    "axes.set_xlabel(\"Gender\")\n",
    "\n",
    "\n",
    "# Multiple graphs\n",
    "x = np.arange(len(table.columns))\n",
    "width = 0.4\n",
    "\n",
    "fig, axes = plt.subplots(2, figsize = (10, 10))\n",
    "axes[0].bar(x - width/2, table.iloc[0], width, label = \"female\")\n",
    "axes[0].bar(x + width/2, table.iloc[1], width, label = \"male\")\n",
    "\n",
    "axes[0].legend()\n",
    "axes[0].set_xticks(x, table.columns)\n",
    "axes[0].set_ylabel(\"Number of People\")\n",
    "axes[0].set_xlabel(\"Workclass\")\n",
    "\n",
    "\n",
    "axes[1].bar(table.columns, table.iloc[0], width, label = \"female\")\n",
    "axes[1].bar(table.columns, table.iloc[1], width, bottom = table.iloc[0], label = \"male\")\n",
    "axes[1].set_ylabel(\"Number of People\")\n",
    "axes[1].set_xlabel(\"Workclass\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bc3253",
   "metadata": {},
   "source": [
    "Barplots: way to display a single categorical variable \n",
    "Realitive frequency bar plot: proportions instead of frequencies are shown \n",
    "\n",
    "Barplot vs histogram?\n",
    "Bar plots are used for displaying distributions of catgorical variable, while histograms are used for numerical variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7159f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data = df, x = 'day',y='bikecount', hue='place')\n",
    "# Grouped Bar Chart\n",
    "axes[0].bar(x - width/2, table.iloc[0], width, label = \"female\")\n",
    "axes[0].bar(x + width/2, table.iloc[1], width, label = \"male\")\n",
    "\n",
    "# Stacked Bar Chart\n",
    "axes[1].bar(table.columns, table.iloc[0], width, label = \"female\")\n",
    "axes[1].bar(table.columns, table.iloc[1], width, bottom = table.iloc[0], label = \"male\")\n",
    "\n",
    "g = table.plot(kind = \"bar\", stacked = True)\n",
    "\n",
    "# Histogram\n",
    "sns.histplot(data = df, x = incomeMales, ax = axes, color = \"b\", bins = 100, alpha = 0.3)\n",
    "axes[0,1].hist(df['educational-num'], bins = 16, color = 'g')\n",
    "\n",
    "# Box Plot \n",
    "sns.boxplot(data = df, y = \"income\", x = \"gender\", ax = axes, color = \"b\")\n",
    "\n",
    "# Violin Plot\n",
    "sns.violinplot(data = df, y = \"income\", x = \"gender\", ax = axes, color = \"y\")\n",
    "sns.violinplot(data = df, y = \"income\", x = \"workclass\", hue = \"gender\", ax = axes, color = \"r\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec408f0",
   "metadata": {},
   "source": [
    "## Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9c58c4",
   "metadata": {},
   "source": [
    "General **Addition rule**: P(A or B) = P(A) + P(B) - P(A and B)\n",
    "\n",
    "**Disjoint Events:** Events that are mutally exclusive, if one happens the other can't  \n",
    "--> P(A or B) = P(A) + P(B)\n",
    "\n",
    "**Marginal Probabilty:** The probability of an event occuring   \n",
    "(total of a category / total of all)  ==> P(A)\n",
    "\n",
    "**Joint Probability:** The likelihood of two events occurring together   \n",
    "(Crosstable value of 2 events / total of all)  ==> P(A and B)\n",
    "\n",
    "**Conditional Probability:** Outcome of intrest A given condition B  \n",
    "-- What you're given is the row/colum you look at  \n",
    "(crosstable value of 2 events / total of all) / (total of a category / total of all)  \n",
    "*(crosstable value of 2 events / total of a category)  ==> P(A|B) = P(A and B) / P(B)*\n",
    "\n",
    "**Multiplication Rule**: P(A and B) = P(A|B) * P(B) (OR P(B|A) * P(A))\n",
    "Condition probability rearranged  \n",
    "Useful to think of A as the outcome of interest and B as the condition\n",
    "\n",
    "**Independence:** Giving B doesn't say anything about A  \n",
    "--> P(A|B) = P(A)  \n",
    "If A and B are independent: P(A and B) = P(A) * P(B)  \n",
    "--> Joint is product of marginals\n",
    "\n",
    "Bayes Rule: P(A|B) = ( P(B|A) * P(A) ) / P(B)\n",
    "\n",
    "\n",
    "Inverting probabilities: See lecture 2 slides for tree breakdown + note on bayes theorem \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cbde34",
   "metadata": {},
   "source": [
    "### Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5d897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "incomeMales = df['income'].groupby(df['gender']).get_group(\"Male\")\n",
    "meanIncomeMales = incomeMales.mean()\n",
    "\n",
    "incomeFemale = df['income'].groupby(df['gender']).get_group(\"Female\")\n",
    "meanIncomeFemales = incomeFemale.mean()\n",
    "\n",
    "\n",
    "menFinishingTimeS = df[\"time\"].groupby(df[\"gender\"]).get_group(\"M\").mean()\n",
    "\n",
    "womenFinishingTimeS = df[\"time\"].groupby(df[\"gender\"]).get_group(\"F\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe84bf67",
   "metadata": {},
   "source": [
    "## Lecture 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ee4bcb",
   "metadata": {},
   "source": [
    "**Confidence interval:** A plausible range of values for the population parameter  \n",
    "The width of the interval contains xx% of possible population means for a given sample mean \n",
    "- A wider interval allows us to be more certain that we capture the population parameter but is not as informative  \n",
    "\n",
    "**Bootstrap:** Resampling (with replacement) from the sample\n",
    "- The distribution of bootstap stats approximate the distribution of the sample stats\n",
    "Pros:\n",
    "- Unversal technique to obtain confidence intervals\n",
    "- Can be applied to any statistics\n",
    "- Confidence intervals are asymptotically correct\n",
    "- Does not make assumptions about the underlying distribution \n",
    "Cons:\n",
    "- Requires programmign skill\n",
    "- Can become noisy with small N \n",
    "- Takes computation time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20b93b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidenceInt(data, prec):\n",
    "    bounds = []\n",
    "    bounds.append(np.percentile(data, 100-prec))\n",
    "    bounds.append(np.percentile(data, prec))\n",
    "    \n",
    "    return bounds\n",
    "\n",
    "# Input arguments \n",
    "# - data: data series to resample\n",
    "# - N: Sample size for each iteration \n",
    "# - fcn: function to apply to the sample to get the statistics\n",
    "# - numIter: Number of resamples (should default to 1000) \n",
    "\n",
    "# Output argument: \n",
    "# - Numpy array of size numIter that contains the estimates of the statistics (i.e. the bootstrap sample)\n",
    "\n",
    "def bootstrap(data, N, fcn, numIter = 1000):\n",
    "    sample = np.array([])\n",
    "    \n",
    "    for i in range (0, numIter):\n",
    "        value = np.random.choice(data, replace = True, size = N)\n",
    "       \n",
    "        x = fcn(value)\n",
    "        \n",
    "        sample = np.append(sample, x)\n",
    "        \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80277fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph with bootstrap and confidence int\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = 1, figsize = (20,7))\n",
    "axes.hist(bootSamp, bins = 30, color = 'b')\n",
    "axes.set_xlabel(\"Mean Time\")\n",
    "axes.set_ylabel(\"Frequency\")\n",
    "plt.axvline(x = confidenceInt(bootSamp, 95)[0], color = 'r')\n",
    "plt.axvline(x = confidenceInt(bootSamp, 95)[1], color = 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eb3416",
   "metadata": {},
   "source": [
    "**Standard Error of the mean**: the standard deviation of a set of sample means\n",
    "- a measure of the expected variability of our estimates\n",
    "- When sample size (N) quadrupels, SEM halves  \n",
    "SEM = SD(x) / sqrt(N) (population SD / Sqrt of sample size)\n",
    "\n",
    "The **standard error of means** nearly matches the **standard deviation of the means**. Therefore the assumption is that the standard error of the population can be estimated from the standared deviation of the mean of distributions.\n",
    "\n",
    "We can't get a confidence interval from the SEM alone we alone we also need shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8cde5e",
   "metadata": {},
   "source": [
    "**Central Limit Theorem:** The sampling distribution of means based on large sample size, can be approximated by a normal distribution \n",
    "- If you sum a large nymber of random variables (of any distribution), the sum will have a normal distribution\n",
    "\n",
    "Normal model:\n",
    "\n",
    " $$\\bar{x} \\sim N(mean = \\mu, SE = \\sigma / \\sqrt{n})$$\n",
    " \n",
    " Where SE is the standard error, which is the SD of the sampling distribution\n",
    " \n",
    "Conditions for CLT:\n",
    "Independence: Sampled observations must be independent, difficult to verify but is more likely if\n",
    "- Random sampling/assignment is used\n",
    "- if sampling without replacement, n < 10% of the population  \n",
    "Sample Size/Skew: EIther the population distribution is normal, or if the population distribution is skew, the smaple size is large\n",
    "- the more skewed the population distribution, the larger sample size we need for CLT to apply\n",
    "- for moderately skewed distributions n > 30 is a widely used rule of thumb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fac3fe0",
   "metadata": {},
   "source": [
    "## Lecture 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76ce60d",
   "metadata": {},
   "source": [
    "Explanatory (independent) variables: indicate conditions we can impose on the experimental units  \n",
    "Blocking variables: characteristics that the experimental units come with, that we would like to control for\n",
    "- like stratifying, except used in experimental settings when randomly assigning, as opposed to when sampling   \n",
    "\n",
    "Confounding variable: variable that co-varies with the independent varibale, and which can provide an alternative explination for the experimental effect  \n",
    "\n",
    "Placebo: fake treatment, often used as the control group for medical studies\n",
    "\n",
    "Placebo effect: experimental units showing improvement simply beacsue they believe they are receiving a special treatment  \n",
    "\n",
    "Blinding: when experimental units do not know whether they are in the control or treatment group \n",
    "\n",
    "Double-blind: when both the experimental units and the reasearchers who interact with the patients do not know who is in the control and who is in the treatment group\n",
    "\n",
    "Null hypothesis: There is nothing going on (x is independent)  \n",
    "Alternative hypothesis: There is something going on (x is dependent)  \n",
    "\n",
    "Randomization methods:\n",
    "- **Randomization/Permutation Test:** Redo random assignemnt performed in the experiment K times or Check all possible ways of the random assignment\n",
    "- **Monte-Carlo Simulation:** generate new data under a parametric model for the Null-Hypothesis \n",
    "- **Bootstrap:** use our sample as a model for the population and draw K new samples of size N \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef9dea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomization / Permutation\n",
    "\n",
    "def permutation_test(frame, fnc, shuffle, numIter = 500, sides = 1, pBin = 25):\n",
    "    sample = np.array([])\n",
    "    \n",
    "    for i in range(0, numIter):\n",
    "        df_copy = frame.copy()\n",
    "        idx = df_copy.index\n",
    "        idx_arr = np.arange(0, len(idx))\n",
    "        np.random.shuffle(idx_arr)\n",
    "        shuffled_gender = df_copy[shuffle].iloc[idx_arr]\n",
    "        shuffled_gender_id = shuffled_gender.reset_index(drop = True)\n",
    "        df_copy[shuffle] = shuffled_gender_id\n",
    "        sample = np.append(sample, fnc(df_copy))\n",
    "        \n",
    "    # Graph     \n",
    "    fig, axes = plt.subplots(nrows = 1, ncols = 1, figsize = (20,10))\n",
    "    axes.hist(sample, bins = pBin, color = 'b')\n",
    "    plt.axvline(x = fnc(frame), color = 'r')\n",
    "    axes.set_ylabel(\"Number of Observations\")\n",
    "    axes.set_xlabel(f\"Result of test\")\n",
    "    \n",
    "    # P-Value\n",
    "    if sides == 2:\n",
    "        plt.axvline(x = -(fnc(frame)), color = 'r')\n",
    "        a = np.absolute(sample) >= fnc(frame)\n",
    "        twosided = sum(a) / len(sample)\n",
    "    \n",
    "        print(f\"The result of the two sided test is {twosided}\")\n",
    "        \n",
    "    else:\n",
    "        a = sample >= fnc(frame)\n",
    "        p = sum(a) / len(sample)\n",
    "        print(f\"The p-value is {p}\")\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b98ac4e",
   "metadata": {},
   "source": [
    "**Errors:**\n",
    "- **Type 1 Error** is rejecting the null hypothesis when H0 is true. \n",
    "- The *probability of the the type one error is the p-value* p(stats >thres|H0)\n",
    "- If making a Type 1 error is dangerous of especially costly, we should choose a small significance level (ex. 0.01) \n",
    "    - We want to be very cautious about rejcting the nul hypothesis, so we demand very strong evidence favouring HA before we would reject H0\n",
    "\n",
    "\n",
    "\n",
    "- **Type 2 Error** is failing to reject the null hypothesis when HA is true\n",
    "- The p-value does not tell us anything about this \n",
    "- If making a type 2 error is relatively more dangerous or much more costly than a type 1 error than we should choose a higher significance level (ex. 0.10) \n",
    "    - We want to be cautious about failing to reject H0 when the null is actually false."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076dfd42",
   "metadata": {},
   "source": [
    "## Lecture 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00dbe4e",
   "metadata": {},
   "source": [
    "**Chi-Square Test of Independence:** the X2 test statistic measures the deviation between observed counts for each cell (O) and the expected counts (E) under the null hypothesis\n",
    "\n",
    "$$ X2 =  \\sum_{i} {(O_i - E_i)^2 \\over E_i }$$\n",
    "\n",
    "Expected count(A and B) = P(A) * P(B) / table total \n",
    "\n",
    "We have a test statistic that quantifies how far our data deviates from the prediction of independence (larger statistic = larger deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5246b7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-Squared\n",
    "\n",
    "def X2(data, x = \"genre\", y = \"dirGender\"):\n",
    "    table = pd.crosstab(data[x], data[y])\n",
    "    tabArr = np.array(table)\n",
    "    \n",
    "    nrow, ncol = tabArr.shape\n",
    "    expectArr = np.zeros([nrow, ncol])\n",
    "    \n",
    "    sObservedCol = np.sum(tabArr, axis = 0)\n",
    "    sObservedRow = np.sum(tabArr, axis = 1)\n",
    "\n",
    "    for i in range(0, len(sObservedCol)):\n",
    "\n",
    "        marginalC = sObservedCol[i]\n",
    "\n",
    "        for j in range (0, len(sObservedRow)):\n",
    "\n",
    "            marginalH = sObservedRow[j]\n",
    "\n",
    "            expectArr[j,i] = (marginalH * marginalC) / np.sum(tabArr)\n",
    "            \n",
    "    X2 = np.sum((tabArr - expectArr)**2/expectArr)\n",
    "    return X2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118182f9",
   "metadata": {},
   "source": [
    "**Monte-Carlo Test:**\n",
    "- Simulation done under the null hypothesis\n",
    "- Count how many cases a value equal or smaller than the measured value is produced "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555fbf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte-Carlo Test\n",
    "\n",
    "# prob = probability of a success on each coin toss\n",
    "# N = the number of coin tosses per trial (N)\n",
    "# t = the number of trials\n",
    "\n",
    "# Simulates COIN TOSSES\n",
    "def monteCarloSim(prob, N, t):\n",
    "\n",
    "    numHead = np.array([])\n",
    "\n",
    "    for trial in range(0, t):\n",
    "        sim = np.random.choice([0,1], size = N, replace = True, p = prob) # p is probability per element\n",
    "        numHead = np.append(numHead, np.sum(sim))\n",
    "    \n",
    "    return numHead\n",
    "\n",
    "\n",
    "# H0prob = The probability of heads under the Null hypothesis\n",
    "# N = The number of coin throws per trial \n",
    "# numHeads = The number of observed coin tosses. \n",
    "\n",
    "# Generates GRAPH\n",
    "def monteCarloTest(H0prob, N, numHeads):\n",
    "    data = monteCarloSim(H0prob, N, 2000)\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows = 1, ncols = 1, figsize = (20,10))\n",
    "    axes.hist(data, bins = 15, color = 'b')\n",
    "    plt.axvline(x = numHeads, color = 'r')\n",
    "    axes.set_ylabel(\"Number of Observations\")\n",
    "    axes.set_xlabel(f\"Number of Complications\")\n",
    "    \n",
    "    a = data <= numHeads\n",
    "    p = sum(a) / len(data)\n",
    "    print(f\"The p-value is {p}\")\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea26961",
   "metadata": {},
   "source": [
    "**Power:** the probability that the test correctly rejects the null hypothesis when a specific alternative hypothesis is true\n",
    "- Need to simulate data under the HA\n",
    "- Need to specify the HA precisely\n",
    "- The difference between the H0 and the HA is called the **effect size**\n",
    "- As power decreases type 1 error decreases and type 2 error increases \n",
    "- Tells us whether we had a good chance of detecting an effect of a specific size\n",
    "- If we are under-powered we can: \n",
    "    - Decrease our threshold and accept a higher type 1 error\n",
    "    - Increase our sample to be appropriately powered\n",
    "    \n",
    "**P-Value is the probability of rejecting H0 if the H0 is true  \n",
    "Power is the probability of rejecting the H0 is the HA is true**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6884b00d",
   "metadata": {},
   "source": [
    "## Lecture 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e1ab91",
   "metadata": {},
   "source": [
    "**Bayesian Inference:** does not use a decision criterion. Only care about the data we observed.\n",
    "- The prior makes inderence subjective\n",
    "- The evidence can be seen by two different people and the posteriors can be very different\n",
    "- While that is reflecting what truly happens, it is not desirable for reporting results objectively \n",
    "\n",
    "**Bayes Factor**:Tells you how much you need to update your prior belief about the probabilities, so it quantifies the evidence independent of priors  \n",
    "Can be interpreted as a betting ratio that would give you a fair bet  \n",
    "Bayesian updating: how to update the ratio of your beliefs\n",
    "\n",
    "$$ BF_{10} = { p(Data|H_A) \\over p(Data|H_0)}$$\n",
    "\n",
    "**Bayesian hypothesis testing:** \n",
    "- Treats H0 and HA the same way\n",
    "   - if BF10 is < 1, look at BF01 = 1/BF10\n",
    "- All hypotheses need to be fully specified\n",
    "- Conclusions depend on the prior probability with which each hypotheses is true\n",
    "\n",
    "| Bayes Factor        | Evidence         |\n",
    "|---------|--------------------|\n",
    "| <1    | Negative, in favour of the other hypothesis |\n",
    "| 1-3 | Barely worth mentioning |\n",
    "| 3-20     | Positive              | \n",
    "| 20-150| Strong|\n",
    "|>150| Very Strong|  \n",
    "\n",
    "Bayesian Inference for continous variables:\n",
    "- Do a randomization test to get the SD of the statistic under the H0 \n",
    "- Assume that p(stats|H0) is noraml (0, SD)\n",
    "- Assume that p(ststs|HA) is normal(effect size, SD)\n",
    "- Look up the corresponding probability densities on using ss.normal.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1f2ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesFactor(probH0, probH1, N, t, obs, prior):\n",
    "\n",
    "    H0numHead = np.array([])\n",
    "    H1numHead = np.array([])\n",
    "\n",
    "    # Coin Tosses\n",
    "    for trial in range(0, t):\n",
    "        H0sim = np.random.choice([0,1], size = N, replace = True, p = [1-probH0, probH0]) # p is probability per element\n",
    "        H0numHead = np.append(H0numHead, np.sum(H0sim))\n",
    "        \n",
    "        H1sim = np.random.choice([0,1], size = N, replace = True, p = [1-probH1, probH1]) # p is probability per element\n",
    "        H1numHead = np.append(H1numHead, np.sum(H1sim))\n",
    "        \n",
    "#         H0numHead = np.append(H0numHead, np.random.binomial(N, probH0))\n",
    "#         H1numHead = np.append(H1numHead, np.random.binomial(N, probH1))\n",
    "      \n",
    "    # Graphs \n",
    "    ax = sns.distplot(H0numHead, color = 'r')\n",
    "    sns.distplot(H1numHead, ax = ax, color = 'b')\n",
    "    plt.axvline(x = obs, color = \"black\")\n",
    "\n",
    "    # P-Value \n",
    "    #Probability of rejecting H0 if H0 is true\n",
    "    \n",
    "    pValue = sum(H0numHead < obs)/len(H0numHead)\n",
    "    print(f\"The p-value is {pValue}\")\n",
    "    \n",
    "    if pValue <= 0.05:\n",
    "        print(\"The null hypothesis was rejected due to the low p-value\")\n",
    "    else:\n",
    "        print(\"There was not sufficient evidence found to reject the null hypothesis\")\n",
    "    \n",
    "    # Power\n",
    "    # prbability of rejectign H0 if HA is true\n",
    "    \n",
    "    power = sum(H1numHead < obs) / len(H1numHead)\n",
    "    print(f\"The power is {power}\")\n",
    "    \n",
    "    if power >= 0.80:\n",
    "        print(\"The null hypothesis was rejected due to the high p-value\")\n",
    "    else:\n",
    "        print(\"There was not sufficient evidence found to reject the null hypothesis\")\n",
    "    \n",
    "    \n",
    "    # Bayes Factor\n",
    "    # BF10 = p(Data|H1)/p(Data|H0)\n",
    "    \n",
    "\n",
    "    # Read from the alternative distribution \n",
    "    BF1 = sum(H1numHead == obs)/len(H1numHead)\n",
    "\n",
    "    # p(observign 8 side effects given | H0)\n",
    "\n",
    "    # Read from the null distribution\n",
    "    BF0 = sum(H0numHead == obs)/len(H0numHead)\n",
    "\n",
    "    BF10 = BF1/BF0\n",
    "\n",
    "    print(f\"\\nThe bayes factor is {BF10}\")\n",
    "    \n",
    "    \n",
    "    # Posterior Probability\n",
    "    \n",
    "    prior0 = prior\n",
    "    prior1 = 1 - prior\n",
    "    posterior1 = (prior1*BF1)/(prior1*BF1 + prior0*BF0)\n",
    "    print(f\"The prosterior probability of the alternative hypothesis is {posterior1}\")\n",
    "    \n",
    "    \n",
    "\n",
    "# Continous Variable Bayesian \n",
    "p0 = ss.norm.pdf(meanDiff(df), 0, STD)\n",
    "\n",
    "p1 = ss.norm.pdf(meanDiff(df), effect, STD)\n",
    "\n",
    "# effect = 0.5?\n",
    "\n",
    "# Calculate BF10\n",
    "BF10 = p1/p0\n",
    "print(BF10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd97ecf",
   "metadata": {},
   "source": [
    "Posterior Probability: the revised or updated probability of an event occurring after taking into consideration new information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd64ed6",
   "metadata": {},
   "source": [
    "## Lecture 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdf242b",
   "metadata": {},
   "source": [
    "### Simple Linear Regression\n",
    "A simple linear regression model follow y = mx + b or y = b0 + b1x  \n",
    "\n",
    "Loss funtion contains the criterion of what consitutes a good and bad fit (RSS = loss)\n",
    "- High loss = bad fit\n",
    "- Low loss = good fit\n",
    "- We aim to pick parameters than minimize loss  \n",
    "If intercept and slope are negative, increase values to get the lowest rss (closest to 0)\n",
    "If intercept and slope are positive, decrease values to get the lowest rss (closest to 0)\n",
    "\n",
    "**Residuals**: are the leftovers from the model (R = y_act - y_pred)\n",
    "- Each data point gives a residual \n",
    "\n",
    "We aim for a line that has small residuals \n",
    "- **Option 1**: Minimize the sum of the absolute value of residuals\n",
    "- **Option 2**: Minimize the sum of squared residuals  \n",
    "\n",
    "Least sqaured is more commonly used for a few reasons\n",
    "1. Motivated by normal distribution of errors\n",
    "2. Solutions can be easily computed\n",
    "3. Big errors count relatively more than small errors\n",
    "\n",
    "In linear regression we have two parameters making the loss-function a surface.  \n",
    "We can speed up fitting a lot if we provide the derivitive of the parameters.\n",
    "##### ADD MATH TO PAPER CHEAT CHEET\n",
    "\n",
    "The vector of partial derivatives is called a gradient our loss function returns the rss (loss) adn the gradient (b values)\n",
    "\n",
    "Quality of fit is assessed using R2, also called the coefficient of determination.  \n",
    "R2 is calculated from the ration of rss to tss.  \n",
    "It tells us what percent of variability in the response variable is explained by the model (0 = no fit, 1 = perfect fit)  \n",
    "The remainder of the variability is explained by variables not incuded in the model or by inherent randomness in the data  \n",
    "\n",
    "**Outliers**: points that lie away from the cloud of points  \n",
    "Outliers that lie horizontally away from the center of the cloud are called high or lowleverage points. These points may influence the slope of the regression line and are called influential points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7634bbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleRegPredict(b, x):\n",
    "    yPredicted = b[0] + b[1]*x\n",
    "    return yPredicted\n",
    "\n",
    "def plotPrediction(b,x,y,fcn = simpleRegPredict):\n",
    "    plt.scatter(x, y) \n",
    "    xPredict = np.linspace(min(x), max(x), num = len(x))\n",
    "    plt.plot(xPredict, fcn(b, xPredict), color = 'red')\n",
    "\n",
    "def simpleRegLossRSS(b, x, y):\n",
    "    yPredict = simpleRegPredict(b, x)\n",
    "\n",
    "    # calculate the residuals (difference between the real and predicted y values)\n",
    "    res = y - yPredict\n",
    "    rss = res**2\n",
    "    loss = np.sum(rss)\n",
    "\n",
    "    # calculate the derivatives with respect to each parameter (regression coefficient)\n",
    "    db0 = np.sum(-2*(res))\n",
    "    db1 = np.sum(-2*x*res)\n",
    "    \n",
    "    #dloss/db\n",
    "    deriv = [db0, db1]\n",
    "    \n",
    "    return loss, deriv\n",
    "\n",
    "def simpleRegFit(x, y):\n",
    "    b0 = [0,0]\n",
    "    \n",
    "    result = so.minimize(simpleRegLossRSS, b0, args=(x, y), jac=True)\n",
    "    \n",
    "    b = result.x\n",
    "    plotPrediction(b, x, y, fcn = simpleRegPredict)\n",
    "    \n",
    "    TSS = sum((y - np.mean(y))**2)\n",
    "    \n",
    "    yPredict = b[0] + b[1]*x\n",
    "    res = y - yPredict\n",
    "    rss = res**2\n",
    "    loss = np.sum(rss)\n",
    "    \n",
    "    R2 = 1 - (loss/TSS)\n",
    "\n",
    "    return R2, b\n",
    "\n",
    "def simpleRegFit(x, y, fcn = simpleRegLossRSS):\n",
    "    b0 = [0,0]\n",
    "    \n",
    "    result = so.minimize(fcn, b0, args=(x, y), jac=True)\n",
    "    \n",
    "    b = result.x\n",
    "    plotPrediction(b, x, y, fcn = simpleRegPredict)\n",
    "    \n",
    "    TSS = sum((y - np.mean(y))**2)\n",
    "    \n",
    "    loss, d = fcn(b,x,y)\n",
    "    \n",
    "    R2 = 1 - (loss/TSS)\n",
    "\n",
    "    return R2, b\n",
    "\n",
    "\n",
    "#OUTLIER EXCLUSION - Likely not needed \n",
    "df2 = df\n",
    "\n",
    "q75 = np.percentile(df2['tailL'], 75, interpolation = 'midpoint')\n",
    "q25 = np.percentile(df2['tailL'], 25, interpolation = 'midpoint')\n",
    "\n",
    "IQR = q75 - q25\n",
    "upper = q75 +1.5*IQR\n",
    "lower = q25 - 1.5*IQR\n",
    "\n",
    "upper_array=np.array(df2['tailL'] >= upper)\n",
    "\n",
    "b = list(upper_array)\n",
    "\n",
    "lower_array=np.array(df2['tailL'] <= lower)\n",
    "\n",
    "c = list(lower_array)\n",
    "\n",
    "for i in range(len(b)):\n",
    "    if b[i] == True:\n",
    "        df2.drop(i, inplace = True)\n",
    "        \n",
    "for i in range(len(c)):\n",
    "    if c[i] == True:\n",
    "        df2.drop(i, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc29519",
   "metadata": {},
   "source": [
    "## Lecture 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc141ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleRegLossSAD(b, x, y):\n",
    "    \n",
    "    yPredict = simpleRegPredict(b, x)\n",
    "\n",
    "    # calculate the residuals (absolute difference between the real and predicted y values)\n",
    "    res = y - yPredict\n",
    "    rss = abs(res)\n",
    "    loss = np.sum(rss)\n",
    "\n",
    "    # calculate the derivatives with respect to each parameter (regression coefficient)\n",
    "    db0 = np.sum(-2*(res))\n",
    "    db1 = np.sum(-2*x*res)\n",
    "    \n",
    "    #dloss/db\n",
    "    deriv = [db0, db1]\n",
    "    \n",
    "    return loss, deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16582e3",
   "metadata": {},
   "source": [
    "### Polynomial regression and Cross Validation \n",
    "Robust statistics aim to provide methods that emulate popular statistical models but are not unduly affected by outliers or other small departures from model assumptions\n",
    "- **Mean** is a measure of central tendency that is sensistve to outliers / strong skew\n",
    "- **Median** is a measure of central tendency that is robust against outliers\n",
    "\n",
    "With each polynomial term, we fit the data better. Extrapolation of the fit can change dramatically but you lose the extrapolation ability.  \n",
    "Models with more free parameters are more complex models.  \n",
    "Complex models (usually) fit the data better than simple models (every model is nested in the previous.  \n",
    "Fit is therefore not a good criterion to compare models.  \n",
    "Good models should *predict* new data better.\n",
    "\n",
    "A predictive R2 tells us which proportion of the variance of y can be predicted (rather than fitted).\n",
    "Can be lower than 0, this means a simple mean of the data is a better predictor.  \n",
    "Models of different complexity can be directly compared. The models with ethe best predictive R2 is makeing the best predictions and can be consisdered the most appropriate description of the data\n",
    "\n",
    "Types of Cross validation:\n",
    "- Leave one out: leave each data point out as the test set and use all other points for training\n",
    "- Split half: use half the data set as test set and the other half as training. Then reverse the assignemnt\n",
    "- K-fold: split the data into k parts, each time leave one part out as test set and train on the others.\n",
    "    - K = 2... Split half\n",
    "    - K = N... Leave one out \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04b52bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyRegPredict(b, x):\n",
    "    \n",
    "    yp = np.zeros(x.shape)\n",
    "\n",
    "    for i in np.arange(len(b)): \n",
    "        yp = yp + (b[i]*(x**i))\n",
    "            \n",
    "    return yp\n",
    "\n",
    "def polyRegLossRSS(b, x, y):\n",
    "    \n",
    "    yPredict = polyRegPredict(b, x)\n",
    "\n",
    "    # calculate the residuals (difference between the real and predicted y values)\n",
    "    res = y - yPredict\n",
    "    rss = res**2\n",
    "    loss = np.sum(rss)\n",
    "\n",
    "    # calculate the derivatives with respect to each parameter (regression coefficient)\n",
    "    \n",
    "    deriv = []\n",
    "    for i in range(len(b)):\n",
    "        deriv.append(-2 * sum((y - polyRegPredict(b, x)) * x**i))\n",
    "    \n",
    "    return loss, deriv\n",
    "\n",
    "def polyRegFit(x, y, order, fcn, fig = True):\n",
    "    # Order 0 = order 1... therefore order = order requested + 1\n",
    "    \n",
    "    result = so.minimize(fcn, np.array([0 for i in range(order)]), args=(x, y), jac=True)\n",
    "    \n",
    "    b = result.x\n",
    "    if fig == True:\n",
    "        plotPrediction(b, x, y, fcn = polyRegPredict)\n",
    "    \n",
    "    TSS = sum((y - np.mean(y))**2)\n",
    "    \n",
    "    #yPredict = b[0] + b[1]*x\n",
    "    yPredict = polyRegPredict(b,x)\n",
    "    res = y - yPredict\n",
    "    rss = res**2\n",
    "    loss = np.sum(rss)\n",
    "    \n",
    "    R2 = 1 - (loss/TSS)\n",
    "\n",
    "    return R2, b\n",
    "\n",
    "def leaveOneOutCV(df, y, fitfnc = multiRegFit, predfnc = multiRegPredict, args =()):\n",
    "\n",
    "    # first create an array that represent the index \n",
    "    ind = np.arange(len(df.index))\n",
    "\n",
    "    N = len(df.index)\n",
    "\n",
    "    yp_cv = np.zeros(N)\n",
    "\n",
    "    # use np.array_split to generate indices for folds\n",
    "    folds = np.array_split(ind, N)\n",
    "    \n",
    "    r,b0 = fitfnc(df, y, args)\n",
    "    \n",
    "\n",
    "    for f in np.arange(N): \n",
    "        folds_cp = folds.copy() # creating a copy of the folds array\n",
    "        test_ind = folds[f] # get the indices for test set\n",
    "        df_test  = df.loc[test_ind] # set one fold aside for testing\n",
    "\n",
    "\n",
    "        del folds_cp[f]        # delete the test set indices\n",
    "        train_ind = np.concatenate(folds_cp, axis = 0) # concatenate all the remaining indices into 1 array\n",
    "        df_train  = df.loc[train_ind]\n",
    "        ytrain = y.loc[train_ind]\n",
    "        \n",
    "        r,b = fitfnc(df_train, ytrain, args)\n",
    "        yp_cv[test_ind] = predfnc(b, df_test, args)\n",
    "\n",
    "    # TSS\n",
    "    TSS = sum((y - y.mean())**2)\n",
    "\n",
    "    # cross validated RSS\n",
    "    RSScv = sum((y - yp_cv)**2)\n",
    "\n",
    "    # cross validated R2\n",
    "    R2cv = 1-RSScv/TSS\n",
    "\n",
    "    # fit and predict\n",
    "    yp = predfnc(b0, df, args)\n",
    "\n",
    "    # \n",
    "    TSS = sum((y-y.mean())**2)\n",
    "    RSS = sum((y-yp)**2)\n",
    "    R2  = 1-RSS/TSS\n",
    "    \n",
    "    return R2cv, R2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772e4cfa",
   "metadata": {},
   "source": [
    "## Lecture 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f4b4c1",
   "metadata": {},
   "source": [
    "### Multiple regression analysis\n",
    "In multiple regression analysis the response varibale is explained by a linear combination of multiple explantory variables.\n",
    "- each varable has its own regression coefficient (b)\n",
    "- We alwasy have an intercept so number of parameters = number of explanatory variables + 1 \n",
    "- Polynomial regression is a special form of multiple regression \n",
    "- In general, the response plane is a Q dimensional hyperplane in Q+1 hyperspace\n",
    "\n",
    "|         | Ex 1  | Ex 2 | Ex 3 |\n",
    "|---------|--------------------|---------------|-------|\n",
    "| Visits    | 0.10 | 0.10  | 0.10 |\n",
    "| Weeks | 0.30 | 0.30  | 0.30 |\n",
    "| Visits + weeks | 0.29 | 0.33        | 0.40    |\n",
    "\n",
    "**Ex1**. After accounting for length of pregnacy, the number of visits **do not add predictive value**  \n",
    "**Ex2**. After accounting for length of pregnacy, the number of visits **do add predictive value** but there is some overlapping variance that both regressors can predict  \n",
    "**Ex1**. They are not correlated. **They cannot explain the same part of the variance.**\n",
    "\n",
    "The bootstrap distribution tells us what variability we should expect in the slope parameter, if we took a new random sample from the population.\n",
    "If CI does not include 0 then it can be agreed that there is evidence of a positive influence.\n",
    "\n",
    "Inference on models: We can determine which model is the most appropriate by comparing the (cross-vaildated) predicition performance.\n",
    "- Comparison is at the level of models\n",
    "- Using nested models, we can switch parameters on and off\n",
    "\n",
    "Inference on parameters: Within a model, we can look at the uncertainty of parameters estimates using bootstrap\n",
    "- Evaluation of uncertainty of a specific parameter in the context of the model\n",
    "- We can construct CI usign bootstrap ad check if value of interest is within\n",
    "If the model is good the inference on model and parameters should match.  \n",
    "\n",
    "Positively correlated regressors lead to:\n",
    "- negative correlation of slope estimates\n",
    "- increase in the variability of the estimates \n",
    "Negatively correlated regressors lead to:\n",
    "- positive correlation of slope estimates\n",
    "- increase in the variability of the estimates \n",
    "Adding many correlated regressors increase teh uncertainty of all of them. R2cv will tell you what's most appropriate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02e86af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiRegPredict(b, df, xname):\n",
    "\n",
    "    ## the intercept is the first element of the parameter array multiplied by 1.\n",
    "    yp = np.ones(len(df.index)) * b[0]\n",
    "\n",
    "    for i in range(len(xname)):\n",
    "        xcurrent = df[xname[i]]\n",
    "        yp = yp + b[i+1]*xcurrent # Add each regression value \n",
    "\n",
    "    return yp\n",
    "\n",
    "def multiRegLossRSS(b, df, y, xname):\n",
    "    # 1. calculate the residuals\n",
    "    yp = multiRegPredict(b, df, xname)\n",
    "\n",
    "    res = y - yp\n",
    "    res2 = res ** 2\n",
    "    RSS = sum(res2)\n",
    "    \n",
    "    \n",
    "    # 1. initialize the derivative array\n",
    "    deriv = np.zeros(len(b))\n",
    "\n",
    "    ## the first element will be the derivative in respect to the intercept\n",
    "    deriv[0] = -2*sum(res)\n",
    "    \n",
    "    # 2. build up the array using a for loop \n",
    "    for i in range(0, len(xname)):\n",
    "        deriv[i+1] = -2*np.sum(df[xname[i]]*res)\n",
    "    \n",
    "    return RSS, deriv\n",
    "\n",
    "def multiRegFit(df, y, xname):\n",
    "    b0 = np.zeros(len(xname)+1)\n",
    "    \n",
    "    result = so.minimize(multiRegLossRSS, b0, args=(df, y, xname), jac=True)\n",
    "    \n",
    "    b = result.x\n",
    "    \n",
    "    TSS = sum((y - np.mean(y))**2)\n",
    "\n",
    "    RSS, deriv = multiRegLossRSS(b,df,y,xname)\n",
    "    R2 = 1 - RSS/TSS \n",
    "\n",
    "    return R2, b\n",
    "\n",
    "\n",
    "def bootstrap(df, y, fitfcn, numIter = 500, args=()):\n",
    "\n",
    "    R2, b = fitfcn(df,y,args)\n",
    "\n",
    "    numParam = len(b)\n",
    "    \n",
    "    N = len(df.index)\n",
    "    ind = np.arange(N)\n",
    "\n",
    "    stat = np.zeros((numIter, numParam))\n",
    "    \n",
    "    for i in range (0, numIter):\n",
    "        sample = np.random.choice(ind, N, replace = True)\n",
    "        \n",
    "        r2, stat[i, :] = fitfcn(df.iloc[sample], y[sample], args)\n",
    "        \n",
    "    return stat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea808542",
   "metadata": {},
   "source": [
    "## Lecture 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9205714a",
   "metadata": {},
   "source": [
    "### Logarithmic Regression\n",
    "When coding categorical variables with more than one option, one group plays the role of the comparison group  \n",
    "\n",
    "|         | X0  | X1 | X2 |\n",
    "|---------|--------------------|---------------|-------|\n",
    "| Canada    | 1 | 1 | 0 |\n",
    "| USA | 1 | 0  | 1 |\n",
    "| Mexico | 1 | 0 | 0  |\n",
    "\n",
    "In this example Mexico is the comparison group. So the dummy variables (X1 and X2 are always 0). The intercept X0 is always 1.  \n",
    "b0: mean(Mexico)  \n",
    "b1: mean(Canada) - mean(Mexico)  \n",
    "b2: mean(USA) - mean(Mexico). \n",
    "\n",
    "- Model-based comparison establishes which set of variables is teh most appropriate to explain the dependent variable. \n",
    "- Parameter bootstrap tells you whether the inlfuence of a variable is significant within the context of a specific model\n",
    "- If the specific model that you are using is bad, your inference on the parameters will be bad (nonsensical, overfitting with a large CI)\n",
    "\n",
    "Logistic Functions aim to fit probability values. \n",
    "1. Model the odds-ratio... the ratio of probability of an event occuring/not occuring **p/(1-p)**\n",
    "2. Model the log of the odds-ratio **log(p/(1-p))**\n",
    "\n",
    "Follows a 2 step modelling approach:\n",
    "an = b0 + b1x\n",
    "pn = 1 / (1 + exp(-an))\n",
    "\n",
    "Loss funtion:\n",
    "- Should captue the deviation of the observed values from prediction\n",
    "- Can use squared or absolute error\n",
    "- The statistically principled solution is to try to maximize the probability of the data under the model p(Data|parameters)\n",
    "- p(Data|parameters) is called the likelihood\n",
    "- The parameter values that maximize p(Data|parameters) are called the maximum-likelihood estimates\n",
    "\n",
    "Likelihood: \n",
    "- Likelihood is the product of all the single probabilities (assuming independence of observations - coin toss)\n",
    "- The likelihood when theres a lot of data will get very close to 0 and because it's a product taking deriitives are hard... this is solved by using log \n",
    "\n",
    "Log Loss:\n",
    "- By maximizing the log-likelihood we maximize the likelihood\n",
    "- By setting the loss (L) to the negative log-likelihod we can maximize the likelihood... this approach is called the Log-loss\n",
    "\n",
    "### SEE PAPER CHEETSHEET FOR GRADIENT OF THE LOSS DERIVATIONS\n",
    "\n",
    "Minimizing the RSS loss function maximizes the likelihood of y being normally distributed\n",
    "\n",
    "|         | Linear Regression          | Logistic Regression     |\n",
    "|:--:|:-------------------:|:-------------:|\n",
    "| Prediction function    | linear fcn | logistic of linear fcn      |\n",
    "| Likilihood | Gaussian | Bernulli  |\n",
    "| Loss Function  | RSS: <br /> $(y-p)^2$ | Log-likeli/Log-loss: <br /> $-ylog(p)$ - $(1-y)log(1-p)$ | \n",
    "| Criterion used for CV | \" |  \" |\n",
    "| Better model has  | Lower RSS, Higher R2| Higher log-likelihood | \n",
    "\n",
    "\n",
    "Interpreting log-likelihoods:\n",
    "- Log probability of the data given the model p(Data|Model)\n",
    "- Actual values do not mean much (depends on the scaling of the data), can be large negative or positive numbers\n",
    "- Can only interpret the difference of log-likelihoods (log BF) or ratio of likelihoods (BF)\n",
    "\n",
    "| Bayes Factor        | Log Bayes Factor |Evidence         |\n",
    "|---------|----------|----------|\n",
    "| <1    |  <0 | Negative, in favour of the other hypothesis |\n",
    "| 1-3 | 0-1 | Barely worth mentioning |\n",
    "| 3-20  | 1-3  | Positive              | \n",
    "| 20-150| 3-5  | Strong|\n",
    "|>150| >5  | Very Strong|  \n",
    "\n",
    "- The best fitting model/parameter is the one, for which the likelihood of the data given that model/paramter is the highest \n",
    "- Because the log is a monotonic function, we maximize the likelihod if we maximize the log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a84ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD A DF COLUMN \n",
    "\n",
    "# with math\n",
    "df['bodyL'] = df.apply(lambda x: x['totalL'] - x['tailL'], axis=1)\n",
    "\n",
    "# discrete variable\n",
    "popI = df['pop'] == 'Vic'\n",
    "popI = np.double(popI)\n",
    "df['popI']  = popI\n",
    "\n",
    "# Graph with seperation between discrete variables\n",
    "plt.scatter(df.age[df.sexI==0], df.bodyL[df.sexI==0])\n",
    "plt.scatter(df.age[df.sexI==1], df.bodyL[df.sexI==1])\n",
    "yp = multiRegPredict(b, df,[\"sexI\", \"age\"])\n",
    "plt.plot(df.age[df.sexI==0], yp[df.sexI==0])\n",
    "plt.plot(df.age[df.sexI==1], yp[df.sexI==1])\n",
    "\n",
    "# Logistic regression \n",
    "\n",
    "def logisticRegPredict(b, df, xname):\n",
    "    \n",
    "    # initialize the predicted array (starting from the intercept)\n",
    "    an = np.ones(len(df.index)) * b[0]\n",
    "    \n",
    "    # get the predicted values\n",
    "    for i in range(len(xname)):\n",
    "        an = an + b[i + 1] * df[xname[i]]\n",
    "        \n",
    "    pn = 1 / (1 + np.exp(-an))\n",
    "    \n",
    "    return pn\n",
    "\n",
    "def logisticRegLoss(b, df, y, xname):\n",
    "    # 1. use the prediction function\n",
    "    pn = logisticRegPredict(b, df, xname)\n",
    "\n",
    "    # 2. get the cost:\n",
    "    ## step by step implementation\n",
    "    A = y*np.log(pn)\n",
    "    B = (1-y)\n",
    "    C = np.log(1 - pn)\n",
    "\n",
    "    ## now look at the formula and implement it using A, B, C\n",
    "    L = -1 * sum(A + B * C)\n",
    "\n",
    "    # 3. get the derivatives array\n",
    "    ## initialize with the value for intercept:\n",
    "    deriv = np.zeros(len(xname)+1)\n",
    "    res = y - pn\n",
    "    \n",
    "    deriv[0] = -sum(res) \n",
    "\n",
    "    ## now loop over and get the grad\n",
    "    for i in range(len(xname)):\n",
    "        deriv[i+1]=-np.sum(df[xname[i]]*res)\n",
    "        \n",
    "    return L, deriv # Returns loss (rss equiv) and b \n",
    "\n",
    "def logisticRegFit(df, y, xname, figure = True):\n",
    "    N = len(xname)\n",
    "    b0 = np.zeros(N+1)\n",
    "    \n",
    "    result = so.minimize(logisticRegLoss,b0,args=(df,y,xname),jac=True)\n",
    "    \n",
    "    b = result.x\n",
    "    ll = -result.fun\n",
    "    \n",
    "    if (N == 1) & (figure ==True):\n",
    "        plt.scatter(df[xname], y) \n",
    "        \n",
    "        mini, maxi = min(df[xname[0]]), max(df[xname[0]])\n",
    "        xp = np.arange(mini, maxi, (maxi - mini)/100)\n",
    "        yp = b[1]*xp + b[0]\n",
    "        pn = np.exp(yp)/(1+np.exp(yp))\n",
    "        plt.plot(xp, pn, 'r-')\n",
    "    \n",
    "    \n",
    "    return ll, b # returns log-loss (R2 equiv) and b "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8b6c53",
   "metadata": {},
   "source": [
    "## Lecture 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a1b91a",
   "metadata": {},
   "source": [
    "Information Criteria\n",
    "Instead of CV we can try to estimate the difference between fit and prediction performance\n",
    "- Lower number indicate better model\n",
    "- Differences in AIC/BIC can be interpreted by the bayes table (x2)\n",
    "\n",
    "**Akaike Information Criterion**  \n",
    "$ AIC = -2l + 2k $  \n",
    "Because the log likelihood scales in number of data points (N), the penalty becomes nearly unimportant for large data sets\n",
    "\n",
    "**Bayes Information Criterion**  \n",
    "$ BIC = -2l + klog(N) $. \n",
    "The BIC scales the penalty softly, therefore preferrign slightly simpler models for larger data sets\n",
    "\n",
    "\n",
    "Model Selection:\n",
    "- When the model gets too complex, the predicted R2 goes down, even if you have the correct model (use fewer regressors)\n",
    "- Reality is all models are wrong some are just useful\n",
    "- Search for the best subset of regressors in a whole class of models \n",
    "- You could test all models and select the best on for an exhaustive search or use a restricted model search to find a good model without fitting all of them\n",
    "\n",
    "Forward Model Search: \n",
    "- Start with the intercept and add one explanatory variable at a time in every combination. Only continue down the path that adds R2cv value. \n",
    "Backward Model Search:\n",
    "- Start with the full model and drop one explanatory variable at a time in every combination. Only continue down the path that adds R2cv value.\n",
    "\n",
    "Regularization:\n",
    "- Is a way in which we can use many regressors\n",
    "- Main idea is to ensure that the regression coefficients are small\n",
    "- The easiest way to do this is to penalize the size of the regresson coeffiecients in the loss function \n",
    "- This makes the fit smoother\n",
    "- Because of this, all regressors eed to be on a similar scale, so it is common practice to z-standardize all regressors (subtract mean and divide by SD)\n",
    "    - B0 is not reglarized\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3c07d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KfoldCVmultReg(D,y,xname,K=20):\n",
    "\n",
    "    N = len(y) #Number of observations\n",
    "    yp= np.zeros(N)\n",
    "\n",
    "    # Make an index vector with K folds\n",
    "    ind = np.arange(N)\n",
    "    ind = np.floor(ind/N*K)\n",
    "    \n",
    "    # Get overall model fit \n",
    "    R2,b_all=multRegFit(D,y,xname)\n",
    "\n",
    "    # Loop over the crossvalidation folds \n",
    "    for i in range(K):\n",
    "        r,b=multRegFit(D[ind!=i],y[ind!=i],xname,b0=b_all)\n",
    "        yp[ind==i]=multRegPredict(b,D[ind==i],xname)\n",
    "        \n",
    "    # Calculate crossvalidated model fit \n",
    "    TSS  = sum((y-y.mean())**2)\n",
    "    RSScv = sum((y-yp)**2)\n",
    "    R2cv = 1-RSScv/TSS\n",
    "    return R2cv,R2 \n",
    "\n",
    "def zstandardize(d):\n",
    "    d = (d-d.mean())/d.std()\n",
    "    return d\n",
    "\n",
    "def ridgeLoss(b,D,y,xname, a = 1):\n",
    "    \n",
    "    predY = multRegPredict(b,D,xname)\n",
    "    res = y-predY\n",
    "    rss = sum(res**2)\n",
    "    grad=np.zeros(len(b))\n",
    "    grad[0]=-2*np.sum(res)\n",
    "    \n",
    "    loss = rss + a*(np.sum(b[1:]**2))\n",
    "    \n",
    "    for i in range(len(xname)):\n",
    "        grad[i+1]=-2*np.sum(D[xname[i]]*res) + (2 * a * b[i+1])\n",
    "        \n",
    "    return (loss,grad)\n",
    "\n",
    "def ridgeFit(D,y,xname=[], a = 1, figure=0,b0=[]):\n",
    "    \n",
    "    k=len(xname)+1\n",
    "    if (len(b0)!=k):\n",
    "        b0=np.zeros((k,))\n",
    "    RES = so.minimize(ridgeLoss,b0,args=(D,y,xname,a),jac=True)\n",
    "    b=RES.x # Results\n",
    "    res = y-np.mean(y)\n",
    "    TSS = sum(res**2)\n",
    "    \n",
    "    predY = multRegPredict(b,D,xname)\n",
    "    res = y-predY\n",
    "    RSS = sum(res**2)\n",
    "    \n",
    "    #RSS,deriv = multRegLossRSS(b,D,y,xname)\n",
    "    R2 = 1-RSS/TSS \n",
    "    if (k==2 and figure==1):\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(1,1,1)\n",
    "        ax.scatter(D[xname[0]],y)\n",
    "        xRange=[min(D[xname[0]]),max(D[xname[0]])]\n",
    "        xp=np.arange(xRange[0],xRange[1],(xRange[1]-xRange[0])/50)\n",
    "        yp=b[0]+b[1]*xp\n",
    "        ax.plot(xp,yp,'r-')\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "    return (R2,b)\n",
    "\n",
    "\n",
    "def KfoldCVridge(D, y, xname, K = 20, fitfcn = ridgeFit, param={}, predfcn = multRegPredict, a = 1):\n",
    "\n",
    "    # first create an array that represent the index \n",
    "    ind = np.arange(len(df.index))\n",
    "    \n",
    "    yp_cv = np.zeros(len(df.index))\n",
    "\n",
    "    # use np.array_split to generate indices for folds\n",
    "    folds = np.array_split(ind, K)\n",
    "    \n",
    "    N = len(folds)\n",
    "    \n",
    "    r,b0 = fitfcn(D,y,xname,a,**param)\n",
    "    \n",
    "    for f in np.arange(N): \n",
    "        folds_cp = folds.copy() # creating a copy of the folds array\n",
    "        test_ind = folds[f] # get the indices for test set\n",
    "        df_test  = df.loc[test_ind] # set one fold aside for testing\n",
    "\n",
    "\n",
    "        del folds_cp[f]        # delete the test set indices\n",
    "        train_ind = np.concatenate(folds_cp, axis = 0) # concatenate all the remaining indices into 1 array\n",
    "        df_train  = df.loc[train_ind]\n",
    "        ytrain = y.loc[train_ind]\n",
    "        \n",
    "        r,b = fitfcn(df_train,ytrain,xname,a, **param) # multRegFit(D,y,xname=[],figure=0,b0=[])\n",
    "        yp_cv[test_ind] = predfcn(b, df_test, xname)# **param) # multRegPredict(b,D,xname)\n",
    "        \n",
    "\n",
    "    # TSS\n",
    "    TSS = sum((y - y.mean())**2)\n",
    "\n",
    "    # cross validated RSS\n",
    "    RSScv = sum((y - yp_cv)**2)\n",
    "\n",
    "    # cross validated R2\n",
    "    R2cv = 1-RSScv/TSS\n",
    "\n",
    "    # fit and predict\n",
    "    yp = predfcn(b0, D, xname) # **param)\n",
    "\n",
    "    # \n",
    "    TSS = sum((y-y.mean())**2)\n",
    "    RSS = sum((y-yp)**2)\n",
    "    R2  = 1-RSS/TSS\n",
    "    \n",
    "    return R2cv, R2\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2522a3b6",
   "metadata": {},
   "source": [
    "### Modified Log functions - Not coded by me\n",
    "Improvements\n",
    "1. prevent log(0) errors by making sure that your predicted value never is smaller than 1e-20 or larger than 1-1e-20. (tip you can use the numpy function `clip`)\n",
    "\n",
    "2. Let logisticRegFit take an additional input parameter, telling it whether it should plot a figure or not (figure=1) \n",
    "\n",
    "3. Let logisticRegFit take an additional input parameter, specifying the starting value for the parameters (b0=[]). If b0 is empty, the function should start with a vector off all zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb33034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODIFIED LOG FUNCTIONS \n",
    "\n",
    "def logisticRegPredict(b,D,xname):\n",
    "    yp=np.ones(len(D.index))*b[0]       # Start out with the intercept  \n",
    "    for i in range(len(xname)):\n",
    "        yp=yp+D[xname[i]]*b[i+1]        # Add the prediction of each regressor seperately \n",
    "    p = np.exp(yp)/(1+np.exp(yp))\n",
    "    p = p.clip(1e-12,1-(1e-12))\n",
    "    return p \n",
    "    \n",
    "def logisticRegLoss(b,D,y,xname):\n",
    "    p = logisticRegPredict(b,D,xname)\n",
    "    cost = -y*np.log(p)-(1-y)*np.log(1-p)\n",
    "    N=len(xname)\n",
    "    grad=np.zeros(N+1)\n",
    "    res = y-p\n",
    "    grad[0]=-sum(res)\n",
    "    for i in range(N):\n",
    "        grad[i+1]=-np.sum(D[xname[i]]*res)         # Add each regressor \n",
    "    return (cost.sum(),grad)\n",
    "    \n",
    "def logisticRegFit(D,y,xname,figure=0,b0=[]):\n",
    "    k=len(xname)+1\n",
    "    if (len(b0)!=k):\n",
    "        b0=np.zeros(k)\n",
    "    RES = so.minimize(logisticRegLoss,b0,args=(D,y,xname),jac=True)\n",
    "    b = RES.x\n",
    "    ll = -RES.fun # Negative function value is the log likelihood \n",
    "    p = logisticRegPredict(b,D,xname)\n",
    "    if (k==2 & figure==1):\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(1,1,1)\n",
    "        ax.scatter(D[xname[0]],y)\n",
    "        xRange=[min(D[xname[0]]),max(D[xname[0]])]\n",
    "        xp=np.arange(xRange[0],xRange[1],(xRange[1]-xRange[0])/50)\n",
    "        yp=b[0]+b[1]*xp\n",
    "        pp=np.exp(yp)/(1+np.exp(yp))\n",
    "        ax.plot(xp,pp,'r-')\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "    return (ll,b)\n",
    "\n",
    "\n",
    "def KfoldCVlogisticReg(D,y,xname,K=20,fitfcn=logisticRegFit,predictfcn=logisticRegPredict):\n",
    "    N = len(y) #Number of observations\n",
    "    yp= np.zeros(N)\n",
    "    ind = np.arange(N)\n",
    "    ind = np.floor(ind/N*K)\n",
    "    \n",
    "    # Get overall model fit \n",
    "    LL,b_all=fitfcn(D,y,xname,figure=0)\n",
    "    \n",
    "    # Loop over the crossvalidation folds \n",
    "    for i in range(K):\n",
    "        r,b=fitfcn(D[ind!=i],y[ind!=i],xname,b0=b_all,figure=0)\n",
    "        yp[ind==i]=predictfcn(b,D[ind==i],xname)\n",
    "    LLcv = sum(y*np.log(yp)+(1-y)*np.log(1-yp))\n",
    "    return LLcv,LL "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
